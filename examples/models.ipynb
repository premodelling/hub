{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d518d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob \n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import array\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7c236d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\Joshlin\\Downloads\\infections-develop\\infections-develop\\warehouse\\design\\raw'\n",
    "\n",
    "data = pd.DataFrame()\n",
    "def read_c(csv):\n",
    "    \"\"\"Function to read the filenames present in the folder\n",
    "    Returns the list containing filename \"\"\"\n",
    "    files = pd.read_csv(csv)\n",
    "    files['filename'] = os.path.basename(csv).split('.')[0]\n",
    "    return files\n",
    "\n",
    "data = pd.concat(map(read_c, glob.glob(os.path.join(path, \"*.csv\")))) #creating dataframe by concatnating all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9872f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      20200301\n",
      "1      20200302\n",
      "2      20200303\n",
      "3      20200304\n",
      "4      20200305\n",
      "         ...   \n",
      "670    20211231\n",
      "671    20220101\n",
      "672    20220102\n",
      "673    20220103\n",
      "674    20220104\n",
      "Name: date, Length: 94500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#changing the format of dates from yyyy-mm-dd to yyyymmdd\n",
    "data['date'] = pd.to_datetime(data['date'],format='%Y-%m-%d') \n",
    "data['date'] = data['date'].astype('str')\n",
    "data['date'] = data['date'].str.replace('-','')\n",
    "print(data['date'])\n",
    "\n",
    "#Removing columns \n",
    "data=data.drop(columns=['dailyCases', 'dailyFirstDoseByVaccinationDate', 'dailySecondDoseByVaccinationDate',\n",
    "                   'dailyThirdInjectionByVaccinationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb860c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                           object\n",
       "covidOccupiedBeds             float64\n",
       "covidOccupiedMVBeds           float64\n",
       "estimatedNewAdmissions        float64\n",
       "EDC0-4                        float64\n",
       "EDC5-9                        float64\n",
       "EDC10-14                      float64\n",
       "EDC15-19                      float64\n",
       "EDC20-24                      float64\n",
       "EDC25-29                      float64\n",
       "EDC30-34                      float64\n",
       "EDC35-39                      float64\n",
       "EDC40-44                      float64\n",
       "EDC45-49                      float64\n",
       "EDC50-54                      float64\n",
       "EDC55-59                      float64\n",
       "EDC60-64                      float64\n",
       "EDC65-69                      float64\n",
       "EDC70-74                      float64\n",
       "EDC75-79                      float64\n",
       "EDC80-84                      float64\n",
       "EDC85-89                      float64\n",
       "EDC90+                        float64\n",
       "newDeaths28DaysByDeathDate    float64\n",
       "EDV12-15                      float64\n",
       "EDV16-17                      float64\n",
       "EDV18-24                      float64\n",
       "EDV25-29                      float64\n",
       "EDV30-34                      float64\n",
       "EDV35-39                      float64\n",
       "EDV40-44                      float64\n",
       "EDV45-49                      float64\n",
       "EDV50-54                      float64\n",
       "EDV55-59                      float64\n",
       "EDV60-64                      float64\n",
       "EDV65-69                      float64\n",
       "EDV70-74                      float64\n",
       "EDV75-79                      float64\n",
       "EDV80-84                      float64\n",
       "EDV85-89                      float64\n",
       "EDV90+                        float64\n",
       "filename                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "217597e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.sort_values(by=['filename','date']) # sorting the values to get the data in sequence\n",
    " # datetime to unix conversion\n",
    "data['date'] = pd.to_datetime(data['date'])  # datetime to unix conversion\n",
    "data['new_date'] = (data['date'] - datetime(1970,1,1))\n",
    "data['new_date'] = ( data['new_date'] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "# list of column names for scaling \n",
    "column_name = ['covidOccupiedBeds', 'covidOccupiedMVBeds',\n",
    "       'estimatedNewAdmissions', 'EDC0-4', 'EDC5-9', 'EDC10-14', 'EDC15-19',\n",
    "       'EDC20-24', 'EDC25-29', 'EDC30-34', 'EDC35-39', 'EDC40-44', 'EDC45-49',\n",
    "       'EDC50-54', 'EDC55-59', 'EDC60-64', 'EDC65-69', 'EDC70-74', 'EDC75-79',\n",
    "       'EDC80-84', 'EDC85-89', 'EDC90+',\n",
    "       'newDeaths28DaysByDeathDate', 'EDV12-15', 'EDV16-17',\n",
    "       'EDV18-24', 'EDV25-29', 'EDV30-34', 'EDV35-39', 'EDV40-44', 'EDV45-49',\n",
    "       'EDV50-54', 'EDV55-59', 'EDV60-64', 'EDV65-69', 'EDV70-74', 'EDV75-79',\n",
    "       'EDV80-84', 'EDV85-89', 'EDV90+','new_date']\n",
    "\n",
    "scaler =StandardScaler()\n",
    "data[column_name]=scaler.fit_transform(data[column_name])\n",
    "\n",
    "#coverting categorical filename to lable \n",
    "le = LabelEncoder()\n",
    "#Adding to the table\n",
    "data['filename'] = le.fit_transform(data['filename'])\n",
    "\n",
    "input_data = pd.DataFrame(data.drop('date',axis=1))\n",
    "new_data = input_data\n",
    "n_features = len(new_data.columns)\n",
    "new_data.head()\n",
    "final_data = new_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c287304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequences_creation(data, n):\n",
    "    \"\"\" \n",
    "    Function to split sequence into steps\n",
    "    slices the data so that each input have the sequence of data of n_steps\n",
    "    returns the output containing the steps\n",
    "    \"\"\"\n",
    "    X, y = list(), list()  \n",
    "    for i in range(len(data)):\n",
    "        # sequence range \n",
    "        end = i + n\n",
    "        # check if the range is within the limit\n",
    "        if end > len(data):\n",
    "            break\n",
    "        # slicing the data set \n",
    "        x_value, y_value = data[i:end, :], data[end-1, 2]\n",
    "        X.append(x_value)\n",
    "        y.append(y_value)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4121c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps  = 7     # number of steps denote the number of days in sequence consider as input  \n",
    "X,y = sequences_creation(final_data, n_steps)\n",
    "X= np.array(X)\n",
    "y =np.array(y)\n",
    "\n",
    "# splitting train, test and validation test\n",
    "val_limit = round((len(X)/100)*80)\n",
    "val_2_limit = round((len(X)/100)*10) + val_limit\n",
    "X_train  =  X[:val_limit]\n",
    "y_train  = y[:val_limit]\n",
    "X_val = X[val_limit:val_2_limit]\n",
    "y_val = y[val_limit:val_2_limit]\n",
    "X_test = X[val_2_limit:]\n",
    "y_test = y[val_2_limit:]\n",
    "n_features = X_train.shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67c7ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,GRU,SimpleRNN\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras import initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b1b7871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 7, 50)             18600     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 7, 50)             0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 7, 20)             5680      \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 7, 20)             0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 7, 1)              21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,301\n",
      "Trainable params: 24,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.HeUniform()\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True,input_shape=(n_steps, n_features),kernel_initializer=initializer))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(LSTM(20,activation='relu',return_sequences=True))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(1,'relu'))\n",
    "model.compile(optimizer= Adam(learning_rate= 0.001),loss=tf.keras.losses.MeanSquaredLogarithmicError(),metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77c988a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0443 - mean_absolute_error: 0.5122- ETA: 0s - loss: 0.0443 - mean_absolute_error: 0\n",
      "Epoch 00001: val_loss improved from inf to 0.05237, saving model to weights.h5\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0443 - mean_absolute_error: 0.5120 - val_loss: 0.0524 - val_mean_absolute_error: 0.4876 - lr: 6.2500e-05\n",
      "Epoch 2/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5121\n",
      "Epoch 00002: val_loss improved from 0.05237 to 0.05236, saving model to weights.h5\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5118 - val_loss: 0.0524 - val_mean_absolute_error: 0.4878 - lr: 6.2500e-05\n",
      "Epoch 3/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0443 - mean_absolute_error: 0.5120\n",
      "Epoch 00003: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 39s 16ms/step - loss: 0.0442 - mean_absolute_error: 0.5119 - val_loss: 0.0524 - val_mean_absolute_error: 0.4879 - lr: 6.2500e-05\n",
      "Epoch 4/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5120\n",
      "Epoch 00004: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5119 - val_loss: 0.0524 - val_mean_absolute_error: 0.4880 - lr: 6.2500e-05\n",
      "Epoch 5/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5118\n",
      "Epoch 00005: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 57s 24ms/step - loss: 0.0442 - mean_absolute_error: 0.5118 - val_loss: 0.0524 - val_mean_absolute_error: 0.4877 - lr: 6.2500e-05\n",
      "Epoch 6/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5119\n",
      "Epoch 00006: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5118 - val_loss: 0.0524 - val_mean_absolute_error: 0.4879 - lr: 6.2500e-05\n",
      "Epoch 7/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5120\n",
      "Epoch 00007: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5118 - val_loss: 0.0524 - val_mean_absolute_error: 0.4880 - lr: 6.2500e-05\n",
      "Epoch 8/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5118\n",
      "Epoch 00008: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5118 - val_loss: 0.0524 - val_mean_absolute_error: 0.4879 - lr: 6.2500e-05\n",
      "Epoch 9/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5121\n",
      "Epoch 00009: val_loss did not improve from 0.05236\n",
      "2363/2363 [==============================] - 19s 8ms/step - loss: 0.0442 - mean_absolute_error: 0.5117 - val_loss: 0.0525 - val_mean_absolute_error: 0.4881 - lr: 6.2500e-05\n",
      "Epoch 10/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5119\n",
      "Epoch 00010: val_loss improved from 0.05236 to 0.05231, saving model to weights.h5\n",
      "2363/2363 [==============================] - 19s 8ms/step - loss: 0.0442 - mean_absolute_error: 0.5117 - val_loss: 0.0523 - val_mean_absolute_error: 0.4876 - lr: 6.2500e-05\n",
      "Epoch 11/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5119\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.05231\n",
      "2363/2363 [==============================] - 19s 8ms/step - loss: 0.0442 - mean_absolute_error: 0.5117 - val_loss: 0.0524 - val_mean_absolute_error: 0.4879 - lr: 6.2500e-05\n",
      "Epoch 12/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5121  ETA: 11s - loss: 0.0381 - mean_ - ETA: 1s - loss: 0.0426 - mean_absolute_error: 0 - ETA: 1s - loss: 0.0428 - mean_\n",
      "Epoch 00012: val_loss improved from 0.05231 to 0.05222, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5119 - val_loss: 0.0522 - val_mean_absolute_error: 0.4862 - lr: 3.1250e-05\n",
      "Epoch 13/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5117- ETA: 1s - loss: 0.0428 - mean\n",
      "Epoch 00013: val_loss improved from 0.05222 to 0.05217, saving model to weights.h5\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5117 - val_loss: 0.0522 - val_mean_absolute_error: 0.4859 - lr: 3.1250e-05\n",
      "Epoch 14/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5119\n",
      "Epoch 00014: val_loss improved from 0.05217 to 0.05215, saving model to weights.h5\n",
      "2363/2363 [==============================] - 22s 9ms/step - loss: 0.0442 - mean_absolute_error: 0.5117 - val_loss: 0.0521 - val_mean_absolute_error: 0.4858 - lr: 3.1250e-05\n",
      "Epoch 15/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5116\n",
      "Epoch 00015: val_loss did not improve from 0.05215\n",
      "2363/2363 [==============================] - 23s 10ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0522 - val_mean_absolute_error: 0.4858 - lr: 3.1250e-05\n",
      "Epoch 16/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5119\n",
      "Epoch 00016: val_loss improved from 0.05215 to 0.05215, saving model to weights.h5\n",
      "2363/2363 [==============================] - 23s 10ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0521 - val_mean_absolute_error: 0.4858 - lr: 3.1250e-05\n",
      "Epoch 17/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5116\n",
      "Epoch 00017: val_loss improved from 0.05215 to 0.05214, saving model to weights.h5\n",
      "2363/2363 [==============================] - 22s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0521 - val_mean_absolute_error: 0.4858 - lr: 3.1250e-05\n",
      "Epoch 18/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5117\n",
      "Epoch 00018: val_loss improved from 0.05214 to 0.05213, saving model to weights.h5\n",
      "2363/2363 [==============================] - 23s 10ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0521 - val_mean_absolute_error: 0.4857 - lr: 3.1250e-05\n",
      "Epoch 19/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5120- ETA: 6s - loss: 0.0428 - mean_absolute_er - ETA: \n",
      "Epoch 00019: val_loss did not improve from 0.05213\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0522 - val_mean_absolute_error: 0.4860 - lr: 3.1250e-05\n",
      "Epoch 20/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5118\n",
      "Epoch 00020: val_loss did not improve from 0.05213\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0522 - val_mean_absolute_error: 0.4860 - lr: 3.1250e-05\n",
      "Epoch 21/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.512 - ETA: 0s - loss: 0.0442 - mean_absolute_error: 0.5117\n",
      "Epoch 00021: val_loss did not improve from 0.05213\n",
      "2363/2363 [==============================] - 22s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5117 - val_loss: 0.0521 - val_mean_absolute_error: 0.4858 - lr: 3.1250e-05\n",
      "Epoch 22/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5117\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05213\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0522 - val_mean_absolute_error: 0.4859 - lr: 3.1250e-05\n",
      "Epoch 23/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5119\n",
      "Epoch 00023: val_loss improved from 0.05213 to 0.05208, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5117 - val_loss: 0.0521 - val_mean_absolute_error: 0.4851 - lr: 1.5625e-05\n",
      "Epoch 24/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5118\n",
      "Epoch 00024: val_loss improved from 0.05208 to 0.05207, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 25/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5115\n",
      "Epoch 00025: val_loss improved from 0.05207 to 0.05206, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 26/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5116\n",
      "Epoch 00026: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 27/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5118\n",
      "Epoch 00027: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4848 - lr: 1.5625e-05\n",
      "Epoch 28/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5118\n",
      "Epoch 00028: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 29/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5118\n",
      "Epoch 00029: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5116 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 30/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5117\n",
      "Epoch 00030: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 31/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5116- ETA: 5s - l\n",
      "Epoch 00031: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 19s 8ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 32/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5116\n",
      "Epoch 00032: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4848 - lr: 1.5625e-05\n",
      "Epoch 33/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5117\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.05206\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4849 - lr: 1.5625e-05\n",
      "Epoch 34/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5117- ETA: 1s - loss: 0.0426 - mean\n",
      "Epoch 00034: val_loss improved from 0.05206 to 0.05206, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4846 - lr: 7.8125e-06\n",
      "Epoch 35/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0441 - mean_absolute_error: 0.5115\n",
      "Epoch 00035: val_loss improved from 0.05206 to 0.05206, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0441 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4846 - lr: 7.8125e-06\n",
      "Epoch 36/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5114- ETA: 9s - loss: 0.0430 - mean_absolute_error: 0.51 - ETA: 9s - loss: 0.0 - ETA: 4s - loss: 0.0418 - mean_a\n",
      "Epoch 00036: val_loss improved from 0.05206 to 0.05206, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 37/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5114\n",
      "Epoch 00037: val_loss improved from 0.05206 to 0.05205, saving model to weights.h5\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 38/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5114\n",
      "Epoch 00038: val_loss improved from 0.05205 to 0.05205, saving model to weights.h5\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0520 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 39/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5117- ETA: 4s - loss: 0.042 - E - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.511\n",
      "Epoch 00039: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5113 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 40/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5115- ETA: 8s - \n",
      "Epoch 00040: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 41/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5115\n",
      "Epoch 00041: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 42/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5118\n",
      "Epoch 00042: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 43/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5116- ETA: 8s - loss: 0.0444 - mean_absolut - ETA:  - ETA: 1s - loss: 0.042\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4845 - lr: 7.8125e-06\n",
      "Epoch 44/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5118- ETA: 1s - loss: 0.0431 - mean_abs\n",
      "Epoch 00044: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 21s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4844 - lr: 3.9063e-06\n",
      "Epoch 45/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5117\n",
      "Epoch 00045: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0440 - mean_absolute_error: 0.5115 - val_loss: 0.0521 - val_mean_absolute_error: 0.4844 - lr: 3.9063e-06\n",
      "Epoch 46/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5116\n",
      "Epoch 00046: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 9ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4844 - lr: 3.9063e-06\n",
      "Epoch 47/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5114- ETA: 1s - loss: 0.0426 - mean_a\n",
      "Epoch 00047: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 19s 8ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4844 - lr: 3.9063e-06\n",
      "Epoch 48/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0440 - mean_absolute_error: 0.5114\n",
      "Epoch 00048: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 20s 8ms/step - loss: 0.0440 - mean_absolute_error: 0.5114 - val_loss: 0.0521 - val_mean_absolute_error: 0.4844 - lr: 3.9063e-06\n",
      "Epoch 00048: early stopping\n",
      "Wall time: 17min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ec6659c40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "#X_train = X_train.astype('float32')\n",
    "#X_train = \n",
    "#y_train = np.asarray(y_train).astype('float32')\n",
    "#X_val = np.asarray(X_val).astype('float32')\n",
    "#y_val = np.asarray(y_val).astype('float32')\n",
    "#print(X_train[0])\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,validation_data =(X_val,y_val),callbacks=[es, rlr, mcp, tb],epochs=200,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3380e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_12 (GRU)                (None, 7, 40)             10080     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 7, 40)             0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 7, 30)             6480      \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 7, 30)             0         \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 7, 10)             1260      \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 7, 10)             0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 7, 1)              11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,831\n",
      "Trainable params: 17,831\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0701 - mean_absolute_error: 0.5634\n",
      "Epoch 00001: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 30s 12ms/step - loss: 0.0701 - mean_absolute_error: 0.5631 - val_loss: 0.0611 - val_mean_absolute_error: 0.5211 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0601 - mean_absolute_error: 0.5479- ETA: 1s - loss: 0.0588 - me\n",
      "Epoch 00002: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 26s 11ms/step - loss: 0.0601 - mean_absolute_error: 0.5476 - val_loss: 0.0579 - val_mean_absolute_error: 0.5174 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0569 - mean_absolute_error: 0.5389\n",
      "Epoch 00003: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 38s 16ms/step - loss: 0.0569 - mean_absolute_error: 0.5389 - val_loss: 0.0565 - val_mean_absolute_error: 0.5130 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0555 - mean_absolute_error: 0.5350\n",
      "Epoch 00004: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 40s 17ms/step - loss: 0.0555 - mean_absolute_error: 0.5348 - val_loss: 0.0559 - val_mean_absolute_error: 0.5101 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0546 - mean_absolute_error: 0.5321\n",
      "Epoch 00005: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 26s 11ms/step - loss: 0.0546 - mean_absolute_error: 0.5319 - val_loss: 0.0554 - val_mean_absolute_error: 0.5114 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0542 - mean_absolute_error: 0.5310- ETA: 9s - loss: 0.0544 - mean_absolute_error:  - ETA: 9s - loss: 0.0540 - mean_absolute_error: 0.533 - ETA: 9s - loss: 0.0538 - mean_absolute - ETA: 8s - loss - ETA: 6s - loss: 0.0523 - mean_absolu - ETA: 2s - loss: \n",
      "Epoch 00006: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 12ms/step - loss: 0.0542 - mean_absolute_error: 0.5310 - val_loss: 0.0545 - val_mean_absolute_error: 0.5068 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0537 - mean_absolute_error: 0.5297- ETA: 4s - loss: 0.0502 - mean_a - ETA: 3 - ETA: 0s - loss: 0.0536 - mean_absolute_error: 0.5 - ETA: 0s - loss: 0.0538 - mean_absolute_error\n",
      "Epoch 00007: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 28s 12ms/step - loss: 0.0537 - mean_absolute_error: 0.5297 - val_loss: 0.0557 - val_mean_absolute_error: 0.5102 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0532 - mean_absolute_error: 0.5282- ETA: 0s - loss: 0.0530 - mean_absolute_er\n",
      "Epoch 00008: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 12ms/step - loss: 0.0532 - mean_absolute_error: 0.5282 - val_loss: 0.0563 - val_mean_absolute_error: 0.5123 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0529 - mean_absolute_error: 0.5278- ETA: 3s - loss: 0.0496 - mean_absolute - ETA: 3s - loss: 0.0495 - mean_absolute_error:  - ET\n",
      "Epoch 00009: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 11ms/step - loss: 0.0529 - mean_absolute_error: 0.5278 - val_loss: 0.0560 - val_mean_absolute_error: 0.5089 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0527 - mean_absolute_error: 0.5273  ETA: 10s - loss: 0 - ETA: 8s -\n",
      "Epoch 00010: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 11ms/step - loss: 0.0527 - mean_absolute_error: 0.5271 - val_loss: 0.0567 - val_mean_absolute_error: 0.5108 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0525 - mean_absolute_error: 0.5268\n",
      "Epoch 00011: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 11ms/step - loss: 0.0525 - mean_absolute_error: 0.5267 - val_loss: 0.0562 - val_mean_absolute_error: 0.5105 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0523 - mean_absolute_error: 0.5267\n",
      "Epoch 00012: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 11ms/step - loss: 0.0523 - mean_absolute_error: 0.5264 - val_loss: 0.0554 - val_mean_absolute_error: 0.5056 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0522 - mean_absolute_error: 0.5260  ETA: 10s - loss: 0.0540 - mean_absolute_error: 0.53 - - ETA: 1s - loss: 0.0515 - mean\n",
      "Epoch 00013: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 11ms/step - loss: 0.0522 - mean_absolute_error: 0.5259 - val_loss: 0.0562 - val_mean_absolute_error: 0.5096 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0520 - mean_absolute_error: 0.5255\n",
      "Epoch 00014: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 28s 12ms/step - loss: 0.0520 - mean_absolute_error: 0.5255 - val_loss: 0.0562 - val_mean_absolute_error: 0.5084 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0519 - mean_absolute_error: 0.5253\n",
      "Epoch 00015: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 27s 12ms/step - loss: 0.0519 - mean_absolute_error: 0.5253 - val_loss: 0.0554 - val_mean_absolute_error: 0.5024 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0518 - mean_absolute_error: 0.5256\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 26s 11ms/step - loss: 0.0518 - mean_absolute_error: 0.5254 - val_loss: 0.0555 - val_mean_absolute_error: 0.5065 - lr: 0.0010\n",
      "Epoch 00016: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19ec6637d60>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GRU\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(GRU(40,return_sequences=True,input_shape=(n_steps, n_features),activation='relu',kernel_initializer=tf.keras.initializers.HeUniform()))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(GRU(30,return_sequences=True, activation='relu'))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(GRU(10,activation='relu',return_sequences=True))\n",
    "model_2.add(Dropout(0.1))\n",
    "model_2.add(Dense(1,activation='relu'))\n",
    "model_2.summary()\n",
    "model_2.compile(optimizer= Adam(learning_rate= 0.001),loss= tf.keras.losses.MeanSquaredLogarithmicError(),metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "\n",
    "model_2.fit(X_train,y_train,validation_data =(X_val,y_val),callbacks=[es, rlr, mcp, tb],epochs=200,verbose=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0150e2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0924 - mean_absolute_error: 0.5648\n",
      "Epoch 00001: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 13s 5ms/step - loss: 0.0924 - mean_absolute_error: 0.5647 - val_loss: 0.0635 - val_mean_absolute_error: 0.4787 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0758 - mean_absolute_error: 0.5500\n",
      "Epoch 00002: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0758 - mean_absolute_error: 0.5498 - val_loss: 0.0599 - val_mean_absolute_error: 0.4685 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0716 - mean_absolute_error: 0.5463\n",
      "Epoch 00003: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0716 - mean_absolute_error: 0.5463 - val_loss: 0.0629 - val_mean_absolute_error: 0.4725 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0679 - mean_absolute_error: 0.5428\n",
      "Epoch 00004: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0679 - mean_absolute_error: 0.5424 - val_loss: 0.0712 - val_mean_absolute_error: 0.5259 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0661 - mean_absolute_error: 0.5408\n",
      "Epoch 00005: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0661 - mean_absolute_error: 0.5408 - val_loss: 0.0594 - val_mean_absolute_error: 0.4761 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "2355/2363 [============================>.] - ETA: 0s - loss: 0.0646 - mean_absolute_error: 0.5397\n",
      "Epoch 00006: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0645 - mean_absolute_error: 0.5392 - val_loss: 0.0623 - val_mean_absolute_error: 0.5066 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "2353/2363 [============================>.] - ETA: 0s - loss: 0.0637 - mean_absolute_error: 0.5390\n",
      "Epoch 00007: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0637 - mean_absolute_error: 0.5385 - val_loss: 0.0604 - val_mean_absolute_error: 0.5120 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0630 - mean_absolute_error: 0.5372\n",
      "Epoch 00008: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 51s 21ms/step - loss: 0.0630 - mean_absolute_error: 0.5372 - val_loss: 0.0592 - val_mean_absolute_error: 0.4938 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0618 - mean_absolute_error: 0.5362\n",
      "Epoch 00009: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0618 - mean_absolute_error: 0.5360 - val_loss: 0.0589 - val_mean_absolute_error: 0.4998 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "2352/2363 [============================>.] - ETA: 0s - loss: 0.0613 - mean_absolute_error: 0.5361\n",
      "Epoch 00010: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0612 - mean_absolute_error: 0.5355 - val_loss: 0.0620 - val_mean_absolute_error: 0.5134 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0608 - mean_absolute_error: 0.5350\n",
      "Epoch 00011: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0608 - mean_absolute_error: 0.5350 - val_loss: 0.0585 - val_mean_absolute_error: 0.4986 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0605 - mean_absolute_error: 0.5347\n",
      "Epoch 00012: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0605 - mean_absolute_error: 0.5346 - val_loss: 0.0554 - val_mean_absolute_error: 0.4827 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "2360/2363 [============================>.] - ETA: 0s - loss: 0.0601 - mean_absolute_error: 0.5340\n",
      "Epoch 00013: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0600 - mean_absolute_error: 0.5339 - val_loss: 0.0587 - val_mean_absolute_error: 0.5042 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0600 - mean_absolute_error: 0.5338\n",
      "Epoch 00014: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0600 - mean_absolute_error: 0.5338 - val_loss: 0.0571 - val_mean_absolute_error: 0.4981 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "2353/2363 [============================>.] - ETA: 0s - loss: 0.0597 - mean_absolute_error: 0.5335\n",
      "Epoch 00015: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0596 - mean_absolute_error: 0.5330 - val_loss: 0.0569 - val_mean_absolute_error: 0.4943 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0592 - mean_absolute_error: 0.5326\n",
      "Epoch 00016: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0592 - mean_absolute_error: 0.5322 - val_loss: 0.0547 - val_mean_absolute_error: 0.4862 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0590 - mean_absolute_error: 0.5323\n",
      "Epoch 00017: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0590 - mean_absolute_error: 0.5320 - val_loss: 0.0547 - val_mean_absolute_error: 0.4821 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "2356/2363 [============================>.] - ETA: 0s - loss: 0.0588 - mean_absolute_error: 0.5322\n",
      "Epoch 00018: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0588 - mean_absolute_error: 0.5318 - val_loss: 0.0560 - val_mean_absolute_error: 0.4894 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0586 - mean_absolute_error: 0.5317\n",
      "Epoch 00019: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 12s 5ms/step - loss: 0.0586 - mean_absolute_error: 0.5315 - val_loss: 0.0554 - val_mean_absolute_error: 0.4857 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0580 - mean_absolute_error: 0.5307\n",
      "Epoch 00020: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.5305 - val_loss: 0.0548 - val_mean_absolute_error: 0.4842 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "2363/2363 [==============================] - ETA: 0s - loss: 0.0580 - mean_absolute_error: 0.5300\n",
      "Epoch 00021: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.5300 - val_loss: 0.0534 - val_mean_absolute_error: 0.4685 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "2362/2363 [============================>.] - ETA: 0s - loss: 0.0580 - mean_absolute_error: 0.5300\n",
      "Epoch 00022: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0580 - mean_absolute_error: 0.5300 - val_loss: 0.0535 - val_mean_absolute_error: 0.4778 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "2353/2363 [============================>.] - ETA: 0s - loss: 0.0577 - mean_absolute_error: 0.5299 - ETA: 1s - loss: 0.0558 - mean_abso - ETA: 0s - loss: 0.0576 - mean_absolute_error: 0.53\n",
      "Epoch 00023: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0577 - mean_absolute_error: 0.5295 - val_loss: 0.0550 - val_mean_absolute_error: 0.4884 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "2358/2363 [============================>.] - ETA: 0s - loss: 0.0574 - mean_absolute_error: 0.5298\n",
      "Epoch 00024: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0575 - mean_absolute_error: 0.5295 - val_loss: 0.0555 - val_mean_absolute_error: 0.4883 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "2361/2363 [============================>.] - ETA: 0s - loss: 0.0573 - mean_absolute_error: 0.5286\n",
      "Epoch 00025: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0573 - mean_absolute_error: 0.5285 - val_loss: 0.0544 - val_mean_absolute_error: 0.4835 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "2354/2363 [============================>.] - ETA: 0s - loss: 0.0574 - mean_absolute_error: 0.5298\n",
      "Epoch 00026: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0573 - mean_absolute_error: 0.5294 - val_loss: 0.0567 - val_mean_absolute_error: 0.4986 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "2351/2363 [============================>.] - ETA: 0s - loss: 0.0569 - mean_absolute_error: 0.5286\n",
      "Epoch 00027: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0570 - mean_absolute_error: 0.5283 - val_loss: 0.0545 - val_mean_absolute_error: 0.4844 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "2359/2363 [============================>.] - ETA: 0s - loss: 0.0567 - mean_absolute_error: 0.5285\n",
      "Epoch 00028: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0568 - mean_absolute_error: 0.5283 - val_loss: 0.0535 - val_mean_absolute_error: 0.4773 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "2356/2363 [============================>.] - ETA: 0s - loss: 0.0567 - mean_absolute_error: 0.5284\n",
      "Epoch 00029: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0567 - mean_absolute_error: 0.5280 - val_loss: 0.0540 - val_mean_absolute_error: 0.4862 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "2355/2363 [============================>.] - ETA: 0s - loss: 0.0566 - mean_absolute_error: 0.5280- ETA: 2s -\n",
      "Epoch 00030: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0566 - mean_absolute_error: 0.5276 - val_loss: 0.0539 - val_mean_absolute_error: 0.4794 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "2357/2363 [============================>.] - ETA: 0s - loss: 0.0565 - mean_absolute_error: 0.5280\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05205\n",
      "2363/2363 [==============================] - 11s 5ms/step - loss: 0.0566 - mean_absolute_error: 0.5276 - val_loss: 0.0554 - val_mean_absolute_error: 0.4896 - lr: 0.0010\n",
      "Epoch 00031: early stopping\n",
      "296/296 [==============================] - 1s 2ms/step - loss: 0.0785 - mean_absolute_error: 0.5463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07850394397974014, 0.5463212132453918]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(200,activation='tanh',kernel_initializer=initializer,input_shape=(n_steps, n_features)))\n",
    "model_3.add(Dense(100,activation='tanh'))\n",
    "model_3.add(Dropout(0.1))\n",
    "model_3.add(Dense(50,activation='tanh'))\n",
    "model_3.add(Dropout(0.1))\n",
    "model_3.add(Dense(30,activation='tanh'))\n",
    "model_3.add(Dropout(0.1))\n",
    "model_3.add(Dense(1,activation='relu'))\n",
    "model_3.compile(optimizer=Adam(learning_rate=0.001), loss=tf.keras.losses.MeanSquaredLogarithmicError(), \n",
    "            metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "# fit model\n",
    "model_3.fit(X_train,y_train,validation_data =(X_val,y_val),callbacks=[es, rlr, mcp, tb],epochs=200,verbose=1,shuffle=False)\n",
    "model_3.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2858779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2363/2363 [==============================] - 8s 4ms/step - loss: 0.0437 - mean_absolute_error: 0.5106\n",
      "2363/2363 [==============================] - 8s 4ms/step - loss: 0.0504 - mean_absolute_error: 0.5363\n",
      "2363/2363 [==============================] - 6s 2ms/step - loss: 0.0556 - mean_absolute_error: 0.5310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.055602092295885086, 0.5309653878211975]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM\n",
    "model.evaluate(X_train,y_train)\n",
    "#GRU\n",
    "model_2.evaluate(X_train,y_train)\n",
    "#Neural Network\n",
    "model_3.evaluate(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8826b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/296 [==============================] - 1s 3ms/step - loss: 0.0525 - mean_absolute_error: 0.4894\n",
      "296/296 [==============================] - 1s 3ms/step - loss: 0.0520 - mean_absolute_error: 0.4964\n",
      "296/296 [==============================] - 1s 2ms/step - loss: 0.0561 - mean_absolute_error: 0.4901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.056079525500535965, 0.4901369512081146]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val,y_val)\n",
    "model_2.evaluate(X_val,y_val)\n",
    "model_3.evaluate(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9f0e9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296/296 [==============================] - 1s 4ms/step - loss: 0.0711 - mean_absolute_error: 0.5331\n",
      "296/296 [==============================] - 1s 3ms/step - loss: 0.0699 - mean_absolute_error: 0.5393A: 0s - loss: 0.0707 - mean_absolute_error: 0.54\n",
      "296/296 [==============================] - 1s 3ms/step - loss: 0.0785 - mean_absolute_error: 0.5445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0785084217786789, 0.5444673299789429]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lstm\n",
    "model.evaluate(X_test,y_test)\n",
    "#GRU\n",
    "model_2.evaluate(X_test,y_test)\n",
    "#Neural network\n",
    "model_3.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
